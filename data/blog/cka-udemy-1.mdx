---
title: 'CKA CoreConcepts'
date: '2022-03-03'
tags: ['cka', 'kubernetes', 'etcd']
draft: true
---

# Etcd

Etcd는 쿠버네티스에서 상태를 갖는 분산된 key-value 저장소입니다. 간단하고 신뢰성 있으며, 빠릅니다.

이는 작은 데이터 청크 단위들을 저장하고 검색하는데 용이합니다.

Etcd는 쿠버네티스와 독립적으로 [설치](https://etcd.io/docs/v3.5/install/)할 수도 있습니다.

Git 소스로부터 빌드하여 사용할 수 있습니다.

```bash
$ git clone -b v3.5.0 https://github.com/etcd-io/etcd.git
```

또는 맥사용자라면 brew를 통해 쉽게 다운로드 가능합니다.

```bash
$ brew install etcd
```

이후에 `etcd`를 실행합니다.

```bash
$ etcd
{"level":"info","ts":"2021-09-17T09:19:32.783-0400","caller":"etcdmain/etcd.go:72","msg":... }

# 데이터 넣기
$ etcdctl put greeting "Hello, etcd"
OK

# 데이터 꺼내기
$ etcdctl get greeting
greeting
Hello, etcd
```

그럼 이 etcd가 쿠버네티스에서 갖는 역할은 무엇일까요?

간단하게 요약하면, `kubectl get` 명령을 실행할 때 표시되는 모든 정보는 ETCD에서 가져옵니다. 즉 상태를

쿠버네티스 리소스 업데이트가 일어날 경우, 이 역시 etcd에 반영됩니다.

## etcd 배포 유형

클러스터를 구축하는 방법에 따라 etcd를 다르게 배포합니다.

처음부터 클러스터를 설정할 경우엔 컨트롤플레인 노드에 etcd를 위에서 설치한 방법대로 바이너리부터 직접 다운로드합니다.

두번째는 kubeadm을 이용하여 클러스터를 설정할 경우, etcd를 pod 형태로 배포할 수도 있습니다. 이 경우 kube-system 네임스페이스에 배포합니다.

만약 고가용성 환경에서는 클러스터에 여러 컨트롤플레인 노드가 있을 겁니다. 이 경우에는 etcd 인스턴스가 분산되어 배포됩니다.

# kube-apiserver

kube-api는 쿠버네티스의 기본 관리 구성요소입니다. 클러스터를 구축하면 항상 있죠. 우리가 kubectl 명령어를 실행하면 실제로 kube-apiserver에 도달합니다.

kube-api는 먼저 요청을 인증하고 유효성을 검사합니다. 그리고 검증이되면 수행합니다.

즉 kube-api에서 모든 명령을 통제하므로, 우리는 cli 형태로 명령을 수행하게 할 수도, RestApi를 보내 명령을 수행하게 할 수도 있습니다.

# kube controller manager

kube controller manager는 쿠버네티스 시스템 내의 다양한 구성 요소의 상태를 지속적으로 모니터링하는 프로세스입니다.

즉 우리가 쿠버네티스에서 선언적으로 리소스를 관리하는 것을 가능케하는 역할이죠.

예를 들어, 노드 컨트롤러는 노드의 상태를 5초마다 모니터링하여 하트비트를 수신하고, desired 개수와 맞는지 확인하고 조정합니다. 이 때, 리소스의 상태에 접근하기 위해서는 kube-apiserver에게 요청하여 거쳐서 옵니다.

이런 컨트롤러가 쿠버네티스 리소스마다 존재합니다. (deployment-controller replica-controller namespace-controller 등)

# kube-scheduler

kube-scheduler는 어떤 파드가 어떤 노드로 결정하는 역할만 수행합니다. 실제로 파드를 다른 노드로 운반하는 작업은 kubelet이 합니다. 즉 kube-scheduler는 관리자 노릇을 합니다.

쿠버네티스에는 다양한 크기(CPU, MEM 등)의 노드와 다양한 크기의 파드가 존재합니다. 스케줄러는 이를 적절하게 배치하는 역할을 수행합니다.

예를 들어보면, 아래 그림과 같이 10 CPU를 소모하는 파드가 있습니다.

![](/static/images/blog/cka-udemy-1/1.png)

4개의 노드는 각각 4, 4, 12, 16 CPU의 가용 용량이 있습니다.

스케줄러는 다음과 같은 순서로 파드의 위치를 정합니다.

1. 가용 가능한 노드 필터링
2. 우선순위 높은 노드 랭킹

첫번째로 4 CPU 노드는 해당 파드가 들어가지 못하므로 필터링하여 제외합니다.

남은 노드는 12, 16 CPU인데, 파드가 가진 10 CPU를 소모하고 가용 용량이 더 남아있는 노드를 선호합니다. 즉 16 CPU 노드를 선택하게 됩니다.

이 외에도 스케줄러의 알고리즘을 커스텀하여 자신만의 스케줄러를 만들 수도 있습니다.

# kubelet

kubelet은 클러스터 워커노드에서 모든 작업을 주도합니다. 컨트롤플레인 노드에서는 kubelet에게 파드를 띄우거나, 제거하는 등 여러 명령을 요청합니다.

kubelet이 수행하는 작업은 대표적으로 다음과 같습니다.

1. 워커 노드 생성 시 컨트롤플레인 노드에 등록
2. 컨트롤플레인 노드에 요청받아 파드를 생성
3. 노드와 파드의 상태를 지속적으로 모니터링

# kube-proxy

클러스터의 모든 파드는 물리적으로 분리되어 있는 다른 노드의 파드에 연결할 수 있습니다. 이는 클러스터의 모든 노드에 걸쳐있는 네트워크가 있기 때문입니다.

kube-proxy는 각 워커노드에서 실행되는 프로세스입니다. 이는 새로운 서비스 리소스를 찾습니다.

각 노드의 서비스 IP가 생길 때, 이를 kube-proxy에서 내부 IP 라우팅 테이블을 만들어 관리합니다. 즉 VPC 내부에서 라우팅 테이블을 통하여 다른 노드와 통신하는 것과 같습니다.

# KodeKloud 실습

Udemy의 CKA 강좌를 듣다보면 브라우저를 통해 쿠버네티스를 직접 컨트롤할 수 있는 실습환경을 구성하여 테스트를 볼 수 있습니다.

[kodekloud](https://kodekloud.com/)에서 [CKA Labs](https://kodekloud.com/courses/labs-certified-kubernetes-administrator-with-practice-tests/)강좌에서 테스트할 수 있습니다.

![](/static/images/blog/cka-udemy-1/2.png)

pod test 중 대부분의 문제는 간단합니다. 지금까지 잘 따라왔다면 문제를 확인한 후 바로 풀 수 있습니다.

한가지 흥미로웠던 부분은 쿠버네티스 명령어를 통해 yaml 초기 파일을 생성하는 점이었습니다.

예를 들어 위 실습중 redis 이름을 가지고 redis123 이미지를 토대로 pod를 yaml을 토대로 만들라고 한 문제가 있었는데, yaml을 직접 타이핑하지 않고 다음과 같이 만듭니다.

`$ k run redis --image=redis123 --dry-run=client -o yaml > pod.yaml`

이렇게 하면 dry-run 옵션 때문에 실제로 파드가 생성되지는 않으며, 만약 이를 토대로 생성했을 때 만들어지는 yaml 형식이 pod.yaml에 저장됩니다.

Pod, Replicasets 테스트 완료

![](/static/images/blog/cka-udemy-1/3.png)

# Deployments

실제 실무 환경에서 pod를 띄워서 서비스를 이용하다 보면, H 스케일링을 해야하기 때문에 pod를 직접적으로 띄운다기 보단 replicasets을 통해 수평 확장 스케일링하게 배포합니다.

또한 서비스는 항상 발전하므로 버전 업데이트를 해야합니다. 그렇다면 v1 파드에서 v2 파드로 어떻게 업데이트 할 수 있을까요?

롤링 등 여러 배포 전략을 취할 수 있는 deployments 리소스가 있습니다.

이를 이용해 버전을 업데이트하거나 안전하게 롤백할 수도 있습니다.

# Namespaces

우리가 여태까지 리소스를 생성하게 제거했던 모든 일들은 네임스페이스 내에서 이루어졌습니다. 기본 네임스페이스 이름은 `default` 입니다.

그리고 쿠버네티스 클러스터가 구축될 때 `kube-system` 네임스페이스가 생성되어 관리 됩니다.

네임스페이스는 특정 목적에 맞게 추가하여 이용할 수 있습니다. 예를 들어, dev, staging, prod 환경으로 네임스페이스를 나누어 같은 이름의 리소스를 이용할 수도 있습니다.

기본적으로 논리적인 구획을 만들어 접근을 금지시키지만, 엄밀하게는 네임스페이스간 통신이 불가능한 것은 아닙니다.

# Services

서비스는 쿠버네티스 리소스의 연결을 담당합니다. 파드도 IP가 존재하여 직접 접근이 가능하지만, 파드는 언제 제거되고 새롭게 만들어질지 모릅니다. 따라서 고정 IP 형태의 서비스를 앞단에 두어 리소스 간 느슨한 결합을 가능하게 만들어줍니다.

쿠버네티스 외부에서 내부에 있는 파드에 접근하기 위해서는 NodePort Type의 서비스가 필요합니다. 노드에서부터 할당된 포트를 열어주어, 서비스를 거친후 파드와 이어집니다.

![](/static/images/blog/cka-udemy-1/4.png)

두번째는 ClusterIP입니다. 이는 쿠버네티스에서 내부적인 통신을 할 때 이용됩니다. 그리고 세번째는 로드밸런서입니다. 이는 NodePort보다 확장성 있는 서비스타입인데, 노드가 몇개가 있는지 상관없이 하나의 endpoint에서만 접근할 줄 알면 됩니다.

# References

- https://www.udemy.com/course/certified-kubernetes-administrator-with-practice-tests/
